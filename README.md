# Proyecto Final: Sistema Distribuido de C√≥mputo y Monitoreo

**Estado:** üöÄ Documento Definitivo de Arquitectura v2.0

Este repositorio contiene el proyecto final para la materia Computaci√≥n II. Es un sistema distribuido desarrollado en Python que implementa dos subsistemas paralelos e interconectados:

1. **Sistema A - Grid de C√≥mputo:** Procesa tareas CPU-bound (an√°lisis gen√≥mico) de forma paralela y distribuida.
2. **Sistema B - Sistema de Monitoreo:** Vigila la salud de los nodos del grid en tiempo real.

**Objetivo Acad√©mico:** Aplicar y demostrar dominio de herramientas de bajo/medio nivel (sockets, IPC, asyncio, colas de tareas) para construir un sistema robusto, concurrente y escalable sin frameworks de alto nivel para comunicaci√≥n de red.

---

## üìñ √çndice

* [1. Visi√≥n General del Proyecto](#1-visi√≥n-general-del-proyecto)
* [2. Arquitectura del Sistema](#2-arquitectura-del-sistema)
* [3. Componentes Detallados](#3-componentes-detallados)
* [4. Protocolos de Comunicaci√≥n](#4-protocolos-de-comunicaci√≥n)
* [5. Flujo de Datos Completo](#5-flujo-de-datos-completo)
* [6. Stack Tecnol√≥gico](#6-stack-tecnol√≥gico)
* [7. Despliegue con Docker](#7-despliegue-con-docker)
* [8. Estructura del Repositorio](#8-estructura-del-repositorio)
* [9. Casos de Uso y Escenarios](#9-casos-de-uso-y-escenarios)
* [10. Consideraciones de Implementaci√≥n](#10-consideraciones-de-implementaci√≥n)
* [11. M√©tricas de Performance Esperadas](#11-m√©tricas-de-performance-esperadas)
* [12. Troubleshooting](#12-troubleshooting)
* [13. Contacto y Contribuciones](#13-contacto-y-contribuciones)

---

## 1. Visi√≥n General del Proyecto

### 1.1 El Concepto: "El Director y El Vigilante"

El proyecto consiste en dos sistemas distribuidos que operan simult√°neamente:

**üéº El Director (Sistema A - Grid de C√≥mputo)**
- **Rol:** Coordinar la ejecuci√≥n paralela de tareas computacionales pesadas
- **Responsabilidad:** Garantizar que el *trabajo* se complete correctamente
- **Ejemplo:** Analizar un genoma de 200MB buscando patrones espec√≠ficos

**üëÅÔ∏è El Vigilante (Sistema B - Monitoreo)**
- **Rol:** Supervisar la salud e infraestructura de los nodos de c√≥mputo
- **Responsabilidad:** Garantizar que los *workers* est√©n operativos y saludables
- **Ejemplo:** Detectar cuando un worker cae, se cuelga o consume recursos anormales

### 1.2 Caso de Uso Principal: An√°lisis Gen√≥mico

**Problema:** Buscar todas las ocurrencias de un patr√≥n de ADN (ej: `AGGTCCAT`) en un archivo de secuencia gen√≥mica de 200MB.

**Soluci√≥n Distribuida:**
1. Dividir el archivo en ~2000-4000 chunks de ~50KB-100KB cada uno
2. Distribuir los chunks a m√∫ltiples workers para procesamiento paralelo
3. Mientras procesan, monitorear su estado de salud en tiempo real
4. Agregar los resultados parciales en un resultado final
5. Persistir el resultado y estad√≠sticas en Redis

**Por qu√© este caso de uso:**
- Es CPU-bound (justifica paralelizaci√≥n)
- Es divisible (justifica grid computing)
- Es realista (bioinform√°tica es un dominio real)
- Exige recursos (demuestra necesidad de monitoreo)

---

## 2. Arquitectura del Sistema

### 2.1 Diagrama de Alto Nivel

```
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ   Cliente CLI   ‚îÇ
                              ‚îÇ  (submit_job)   ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ TCP Socket (JSON)
                                       ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     Servidor Master (A)          ‚îÇ
                    ‚îÇ   - Recibe trabajos              ‚îÇ
                    ‚îÇ   - Divide en chunks             ‚îÇ
                    ‚îÇ   - Encola tareas                ‚îÇ
                    ‚îÇ   - Agrega resultados            ‚îÇ
                    ‚îÇ   - Consulta estado workers      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
                           ‚îÇ Publica tareas                    ‚îÇ
                           ‚ñº                                   ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ TCP Socket
                    ‚îÇ   Redis Server   ‚îÇ                       ‚îÇ (Notificaciones)
                    ‚îÇ  - Cola Celery   ‚îÇ                       ‚îÇ
                    ‚îÇ  - Resultados    ‚îÇ                       ‚îÇ
                    ‚îÇ  - Estado        ‚îÇ                       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
                              ‚îÇ                                ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
                ‚îÇ Consume tareas            ‚îÇ                 ‚îÇ
                ‚ñº                           ‚ñº                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
    ‚îÇ   Worker Node 1     ‚îÇ   ‚îÇ   Worker Node N     ‚îÇ        ‚îÇ
    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ        ‚îÇ
    ‚îÇ ‚îÇ Celery Worker   ‚îÇ ‚îÇ   ‚îÇ ‚îÇ Celery Worker   ‚îÇ ‚îÇ        ‚îÇ
    ‚îÇ ‚îÇ (procesa chunks)‚îÇ ‚îÇ   ‚îÇ ‚îÇ (procesa chunks)‚îÇ ‚îÇ        ‚îÇ
    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ        ‚îÇ
    ‚îÇ          ‚îÇ Unix     ‚îÇ   ‚îÇ          ‚îÇ Unix     ‚îÇ        ‚îÇ
    ‚îÇ          ‚îÇ Socket   ‚îÇ   ‚îÇ          ‚îÇ Socket   ‚îÇ        ‚îÇ
    ‚îÇ          ‚îÇ (IPC)    ‚îÇ   ‚îÇ          ‚îÇ (IPC)    ‚îÇ        ‚îÇ
    ‚îÇ          ‚ñº          ‚îÇ   ‚îÇ          ‚ñº          ‚îÇ        ‚îÇ
    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ        ‚îÇ
    ‚îÇ ‚îÇ Agente Monitor  ‚îÇ ‚îÇ   ‚îÇ ‚îÇ Agente Monitor  ‚îÇ ‚îÇ        ‚îÇ
    ‚îÇ ‚îÇ - Lee m√©tricas  ‚îÇ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÇ - Lee m√©tricas  ‚îÇ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ ‚îÇ - Reporta CPU   ‚îÇ ‚îÇ   ‚îÇ ‚îÇ - Reporta CPU   ‚îÇ ‚îÇ
    ‚îÇ ‚îÇ - Heartbeat     ‚îÇ ‚îÇ   ‚îÇ ‚îÇ - Heartbeat     ‚îÇ ‚îÇ
    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                         ‚îÇ
                ‚îÇ TCP Socket (JSON)       ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ  Servidor Collector  ‚îÇ
                  ‚îÇ       (B)            ‚îÇ
                  ‚îÇ - Recibe m√©tricas    ‚îÇ
                  ‚îÇ - Detecta anomal√≠as  ‚îÇ
                  ‚îÇ - Genera alertas     ‚îÇ
                  ‚îÇ - Notifica a Master  ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Topolog√≠a de Red

**Conexiones TCP (Sockets):**
1. `Cliente CLI` ‚Üî `Master` (puerto 5000)
2. `Agentes` ‚Üí `Collector` (puerto 6000)
3. `Collector` ‚Üí `Master` (puerto 5000)

**Conexiones IPC (Unix Domain Sockets):**
- `Worker` ‚Üî `Agente` (mismo contenedor/host)

**Redis:**
- `Master`, `Workers` ‚Üí Redis (puerto 6379)

### 2.3 Flujo de Informaci√≥n

**Comunicaci√≥n Local (IPC - dentro del mismo contenedor):**
- Worker ‚Üî Agente: Unix Domain Socket para heartbeats
- Procesos Celery internos: `multiprocessing.Queue` y `Lock` (solo dentro de cada Worker)

**Comunicaci√≥n Distribuida (TCP - entre contenedores):**
- Cliente ‚Üí Master: Sockets TCP (env√≠o de trabajos)
- Agente ‚Üí Collector: Sockets TCP (reporte de m√©tricas)
- Collector ‚Üí Master: Sockets TCP (notificaciones de alertas)

**Comunicaci√≥n v√≠a Redis:**
- Master ‚Üí Workers: Celery Queue (distribuci√≥n de chunks)
- Workers ‚Üí Redis: Almacenamiento de resultados parciales

```
[Cliente] ---(TCP)---> [Master] ---(Redis)---> [Workers]
                          ‚ñ≤                        ‚îÇ
                          ‚îÇ                        ‚îÇ (IPC local)
                          ‚îÇ                        ‚ñº
                          ‚îÇ                   [Agentes]
                          ‚îÇ                        ‚îÇ
                          ‚îÇ                        ‚îÇ (TCP)
                          ‚îÇ                        ‚ñº
                          ‚îî‚îÄ‚îÄ(TCP alerta)‚îÄ‚îÄ‚îÄ‚îÄ [Collector]
```

---

## 3. Componentes Detallados

### 3.1 Cliente CLI (`submit_job.py`)

**Prop√≥sito:** Interfaz de l√≠nea de comandos para enviar trabajos al grid.

**Tecnolog√≠as:**
- `argparse` para parseo de argumentos
- `socket` para conexi√≥n TCP al Master
- `json` para serializaci√≥n de mensajes

**Argumentos:**
```bash
python submit_job.py \
  --server localhost \
  --port 5000 \
  --file genome.txt \
  --pattern "AGGTCCAT" \
  --chunk-size 51200  # 50KB por chunk (opcional)
```

**Protocolo de Comunicaci√≥n:**
```json
// REQUEST (Cliente -> Master)
{
  "type": "submit_job",
  "job_id": "uuid-generado-por-cliente",
  "filename": "genome.txt",
  "pattern": "AGGTCCAT",
  "chunk_size": 51200,
  "file_size": 209715200,
  "file_data_b64": "base64_encoded_data..."  // Archivo codificado en base64
}

// RESPONSE (Master -> Cliente)
{
  "status": "accepted",
  "job_id": "uuid-generado-por-cliente",
  "total_chunks": 4096,
  "estimated_time": 120  // segundos
}
```

**Nota sobre env√≠o de archivos grandes:**
Para la demo del proyecto, el archivo de 200MB se env√≠a codificado en base64 dentro del JSON (~267MB). Esto es funcional para el alcance acad√©mico del proyecto. En un sistema de producci√≥n, se implementar√≠a streaming por chunks o upload HTTP multiparte para mayor eficiencia.

**Funcionalidades:**
- Validar que el archivo existe
- Calcular tama√±o y estimar chunks
- Enviar archivo al Master (puede hacerse por chunks si es muy grande)
- Recibir confirmaci√≥n con `job_id`
- (Opcional) Consultar progreso: `python query_job.py --job-id <uuid>`

---

### 3.2 Servidor Master (`master_server.py`)

**Prop√≥sito:** Orquestador central del sistema de c√≥mputo.

**Tecnolog√≠as:**
- `asyncio` para servidor as√≠ncrono
- `sockets` (via `asyncio.start_server`) para conexiones TCP
- `celery` para encolar tareas
- `redis` (librer√≠a python) para persistencia

**Responsabilidades:**

1. **Recepci√≥n de Trabajos:**
   - Escuchar en puerto 5000
   - Aceptar m√∫ltiples clientes concurrentemente
   - Validar formato JSON del trabajo

2. **Divisi√≥n en Chunks:**
   - Dividir archivo en bloques de ~50KB
   - Agregar overlap de N bytes entre chunks (para patrones en fronteras)
   - Generar metadatos por chunk (√≠ndice, offset, size)

3. **Encolado en Celery:**
   - Por cada chunk, publicar tarea: `tasks.find_pattern.delay(chunk_data, pattern, metadata)`
   - Guardar mapping `job_id -> [task_ids]` en Redis

4. **Agregaci√≥n de Resultados:**
   - Escuchar resultados de workers (via Redis o callbacks)
   - Agregar conteo de ocurrencias
   - Guardar resultado final en Redis: `result:{job_id}`

5. **Comunicaci√≥n con Collector:**
   - Recibir notificaciones de workers ca√≠dos
   - (Futuro) Re-encolar tareas de workers fallidos

**Estructura del C√≥digo:**

**Persistencia en Redis:**
```
job:{job_id}:tasks -> lista de task_ids
job:{job_id}:status -> "pending" | "processing" | "completed" | "failed"
job:{job_id}:result -> JSON con resultado final
job:{job_id}:stats -> {"total_matches": 42, "chunks_processed": 4096, ...}
```

---

### 3.3 Celery Worker (`genome_worker.py`)

**Prop√≥sito:** Procesar chunks de datos (CPU-bound).

**Tecnolog√≠as:**
- `celery` como framework de workers
- Algoritmo de b√∫squeda de patrones (ej: KMP, Boyer-Moore, o regex)

**Tarea Principal:**

**Configuraci√≥n:**
- Concurrencia: 2-4 procesos por worker (depende de CPUs)
- Prefetch: 2 (no acaparar tareas)
- Ack_late: True (reencolar si worker cae antes de completar)

**Comando de inicio:**
```bash
celery -A genome_worker worker \
  --loglevel=info \
  --concurrency=4 \
  --hostname=worker1@%h
```

---

### 3.4 Agente Monitor (`monitor_agent.py`)

**Prop√≥sito:** Monitorear la salud del worker en su misma m√°quina/contenedor.

**Tecnolog√≠as:**
- `psutil` para m√©tricas del sistema
- `socket` (Unix Domain Socket) para IPC con Worker
- `socket` (TCP) para reportar al Collector
- `asyncio` para loop principal

**Responsabilidades:**

1. **Recolecci√≥n de M√©tricas:**
   - CPU usage (%)
   - RAM usage (MB y %)
   - N√∫mero de tareas procesadas (leer de IPC)

2. **Comunicaci√≥n IPC con Worker:**
   - Crear Unix Domain Socket: `/tmp/worker_{id}.sock`
   - Escuchar heartbeats del worker cada 5 segundos
   - Si no recibe heartbeat en 15 segundos ‚Üí DEAD

3. **Reporte al Collector:**
   - Cada 10 segundos enviar m√©tricas
   - Si detecta estado DEAD, enviar alerta inmediata

**Estructura del C√≥digo:**

**IPC: Heartbeat desde Worker:**

**Nota importante:** El Worker de Celery con `--concurrency=4` lanza 4 procesos hijos. Cada uno puede enviar heartbeats al mismo Unix socket. El Agente, usando `asyncio.start_unix_server()`, acepta m√∫ltiples conexiones concurrentes sin problema.

---

### 3.5 Servidor Collector (`collector_server.py`)

**Prop√≥sito:** Centralizar monitoreo de todos los workers y generar alertas.

**Tecnolog√≠as:**
- `asyncio` para servidor as√≠ncrono
- `sockets` para recibir m√©tricas de agentes
- `celery` para encolar tareas de alerta
- `redis` para guardar estado de workers

**Responsabilidades:**

1. **Recepci√≥n de M√©tricas:**
   - Escuchar en puerto 6000
   - Aceptar conexiones de m√∫ltiples agentes
   - Parsear JSON de m√©tricas

2. **Detecci√≥n de Anomal√≠as:**
   - Status = "DEAD" (reportado por el agente) ‚Üí Alerta cr√≠tica
   - No recibe m√©tricas de un agente por 30 seg (timeout) ‚Üí Alerta cr√≠tica

3. **Generaci√≥n de Alertas:**
   - Loggear la alerta (actualmente imprime en consola)
   - Notificar al Master v√≠a TCP

4. **Persistencia en Redis:**
   - Guardar √∫ltimo estado de cada worker
   - Guardar historial de alertas

**Estructura del C√≥digo:**

---

## 4. Protocolos de Comunicaci√≥n

### 4.1 Formato de Mensajes JSON

Todos los mensajes entre componentes usan JSON con la siguiente estructura base:

```json
{
  "type": "message_type",
  "timestamp": 1234567890.123,
  "data": { ... }
}
```

### 4.2 Mensajes Cliente ‚Üî Master

**Submit Job:**
```json
{
  "type": "submit_job",
  "job_id": "uuid-generado-por-cliente",
  "filename": "genome.txt",
  "pattern": "AGGTCCAT",
  "chunk_size": 51200,
  "file_size": 209715200,
  "file_data_b64": "..."
}
```

**Job Status Query:**
```json
{
  "type": "query_status",
  "job_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

**Response:**
```json
{
  "status": "processing",
  "job_id": "uuid-generado-por-cliente",
  "progress": {
    "total_chunks": 4096,
    "processed_chunks": 2048,
    "percentage": 50.0
  },
  "partial_results": {
    "matches_found": 42
  }
}
```

### 4.3 Mensajes Agente ‚Üí Collector

**Metrics Report:**
```json
{
  "type": "metrics",
  "data": {
    "worker_id": "worker_1",
    "timestamp": 1234567890.123,
    "status": "ALIVE",
    "cpu_percent": 87.5,
    "memory_mb": 1024.5,
    "memory_percent": 45.2,
    "tasks_processed": 128
  }
}
```

### 4.4 Mensajes Collector ‚Üí Master

**Worker Down Notification:**
```json
{
  "type": "worker_down",
  "worker_id": "worker_3",
  "timestamp": 1234567890.123,
  "last_task_id": "abc123-task-id"
}
```

### 4.5 IPC Worker ‚Üî Agente (Unix Socket)

**Heartbeat:**
```json
{
  "type": "heartbeat",
  "timestamp": 1234567890.123,
  "tasks_completed": 10
}
```

---

## 5. Flujo de Datos Completo

### 5.1 Escenario Normal (Happy Path)

1. **T=0s:** Usuario ejecuta `python submit_job.py --file genome.txt --pattern AGGTCCAT`
2. **T=1s:** Cliente se conecta al Master v√≠a TCP, env√≠a JSON con el trabajo
3. **T=2s:** Master recibe, valida y divide archivo en 4096 chunks de 50KB
4. **T=3s:** Master encola 4096 tareas en Celery/Redis
5. **T=4s:** Los 3 Workers comienzan a consumir tareas de la cola
6. **T=5s:** Cada Worker procesa chunks, env√≠a heartbeat a su Agente local cada 5s
7. **T=10s:** Agentes reportan m√©tricas al Collector: CPU 95%, Status ALIVE
8. **T=15s:** Collector recibe m√©tricas, valida, guarda en Redis, no detecta anomal√≠as
9. **T=300s:** Workers completan todas las tareas
10. **T=301s:** Master agrega resultados de Redis, genera resultado final
11. **T=302s:** Master guarda en Redis: `result:{job_id} = {"total_matches": 142, ...}`
12. **T=303s:** Cliente puede consultar resultado con `query_job.py`

### 5.2 Escenario de Fallo (Worker Ca√≠do)

1. **T=0-100s:** Sistema procesando normally
2. **T=100s:** Worker_2 sufre segfault y muere procesando chunk #1537
3. **T=105s:** Agente_2 intenta leer heartbeat via IPC, no recibe respuesta
4. **T=115s:** Agente_2 marca status = DEAD (15s sin heartbeat)
5. **T=120s:** Agente_2 reporta al Collector: Status DEAD
6. **T=121s:** Collector detecta anomal√≠a cr√≠tica, genera alerta
7. **T=122s:** Collector notifica al Master: `worker_down` con worker_id=worker_2
8. **T=123s:** Master loggea el evento: "Worker 2 detectado como ca√≠do"
9. **T=124s:** Celery, gracias a `ack_late=True`, autom√°ticamente re-encola las tareas que worker_2 no complet√≥
10. **T=125s:** Workers 1 y 3 (a√∫n vivos) toman las tareas re-encoladas
11. **T=200s:** Sistema completa procesamiento con 2 workers

**Nota sobre re-encolado:** Celery maneja autom√°ticamente el re-encolado de tareas cuando un worker cae antes de completarlas (configurado con `ack_late=True` y `reject_on_worker_lost=True`). El Master solo necesita registrar el evento para auditor√≠a.

### 5.3 Diagrama de Secuencia (Submit Job)

```
Cliente          Master          Redis           Worker          Agente          Collector
  |                |               |               |               |               |
  |--submit_job--->|               |               |               |               |
  |                |--divide------>|               |               |               |
  |                |--enqueue----->|               |               |               |
  |<--accepted-----|               |               |               |               |
  |                |               |<--get_task----|               |               |
  |                |               |--task-------->|               |               |
  |                |               |               |--heartbeat--->|               |
  |                |               |               |   (IPC)       |               |
  |                |               |               |               |--metrics----->|
  |                |               |               |               |               |--validate-->
  |                |               |<--result------|               |               |
  |                |<--aggregate---|               |               |               |
  |--query-------->|               |               |               |               |
  |<--status-------|               |               |               |               |
```

---

## 6. Cumplimiento de Requisitos

### 6.1 Requisitos Obligatorios de la C√°tedra

| Requisito | Implementaci√≥n | Justificaci√≥n |
|-----------|----------------|---------------|
| **Sockets con m√∫ltiples clientes** | `asyncio.start_server()` en Master y Collector | Master maneja N clientes CLI concurrentemente. Collector maneja N agentes. No se usa framework web, sino sockets directos. |
| **Asincronismo I/O** | `asyncio` con `async/await` | Cr√≠tico para que Master y Collector manejen cientos de conexiones sin bloquearse. Permite I/O concurrente eficiente. |
| **Cola de tareas distribuidas** | `Celery + Redis` | (1) Workers procesan chunks CPU-bound. (2) Collector encola alertas I/O-bound. Demuestra versatilidad del patr√≥n. |
| **Mecanismos IPC** | Unix Domain Sockets | Worker y Agente (procesos distintos, mismo host) se comunican v√≠a socket Unix para heartbeats. Es IPC puro, no red. |
| **Parseo de argumentos CLI** | `argparse` | Cliente, Master, Collector, Worker, Agente: todos usan `argparse` para configuraci√≥n (--port, --host, --worker-id, etc.). |

### 6.2 Requisitos Adicionales Implementados

- **Docker/Docker Compose:** Despliegue completo con 6 contenedores
- **Persistencia:** Redis para resultados, estado de jobs, m√©tricas
- **Logging estructurado:** Logs en JSON para an√°lisis
- **Manejo de errores:** Try-except en todas las operaciones de red/IPC
- **M√©tricas del sistema:** `psutil` para CPU, RAM
- **Protocolo JSON:** Todos los mensajes son JSON bien documentados

---

## 6. Stack Tecnol√≥gico

### 7.1 Tecnolog√≠as Core

| Componente | Versi√≥n | Uso |
|------------|---------|-----|
| **Python** | 3.11+ | Lenguaje principal |
| **asyncio** | stdlib | Servidores as√≠ncronos (Master, Collector) |
| **sockets** | stdlib | Comunicaci√≥n de red de bajo nivel |
| **Celery** | 5.3+ | Cola de tareas distribuidas |
| **Redis** | 7.0+ | (1) Broker Celery, (2) Persistencia, (3) Estado |
| **multiprocessing** | stdlib | IPC con Unix Domain Sockets |
| **argparse** | stdlib | CLI parsing |
| **psutil** | 5.9+ | M√©tricas del sistema |
| **Docker** | 24+ | Contenedorizaci√≥n |

### 7.2 Librer√≠as Python (requirements.txt)

```txt
# Core
celery==5.3.4
redis==5.0.1
psutil==5.9.6

# Opcional (para testing/desarrollo)
pytest==7.4.3
pytest-asyncio==0.21.1
```

### 7.3 √Årbol de Dependencias

```
asyncio (stdlib)
‚îú‚îÄ‚îÄ sockets (stdlib)
‚îî‚îÄ‚îÄ json (stdlib)

Celery
‚îî‚îÄ‚îÄ Redis (broker + backend)

psutil
‚îî‚îÄ‚îÄ (sin dependencias extras)

IPC
‚îî‚îÄ‚îÄ socket (stdlib, AF_UNIX)
```

---

## 7. Despliegue con Docker

### 8.1 Arquitectura de Contenedores

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Docker Network                    ‚îÇ
‚îÇ                  (genome-network)                   ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Master  ‚îÇ  ‚îÇCollector ‚îÇ  ‚îÇ      Redis       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ :5000    ‚îÇ  ‚îÇ :6000    ‚îÇ  ‚îÇ     :6379        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ             ‚îÇ                  ‚îÇ           ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                     ‚îÇ                              ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ       ‚îÇ                           ‚îÇ                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇWorker 1 ‚îÇ  ‚îÇWorker 2  ‚îÇ  ‚îÇWorker 3  ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ  ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ‚îÇAgente ‚îÇ‚îÇ  ‚îÇ‚îÇAgente ‚îÇ ‚îÇ  ‚îÇ‚îÇAgente ‚îÇ ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ  ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 8.2 docker-compose.yml

### 8.3 Dockerfiles

**docker/Dockerfile.master:**

**docker/Dockerfile.collector:**

**docker/Dockerfile.worker:**

### 8.4 Comandos de Despliegue

```bash
# Build de todas las im√°genes
docker-compose build

# Levantar todo el sistema
docker-compose up -d

# Ver logs en tiempo real
docker-compose logs -f

# Ver logs de un componente espec√≠fico
docker-compose logs -f master
docker-compose logs -f worker1

# Escalar workers (agregar m√°s)
docker-compose up -d --scale worker=5

# Detener todo
docker-compose down

# Detener y limpiar vol√∫menes
docker-compose down -v
```

---

## 8. Estructura del Repositorio

```
final/
‚îÇ
‚îú‚îÄ‚îÄ README.md                 # Este documento (gu√≠a principal)
‚îú‚îÄ‚îÄ INSTALL.md               # Instrucciones de instalaci√≥n detalladas
‚îú‚îÄ‚îÄ INFO.md                  # Justificaciones de dise√±o t√©cnico
‚îú‚îÄ‚îÄ TODO.md                  # Mejoras futuras y features pendientes
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt         # Dependencias Python
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.master
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.collector
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile.worker
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml       # Orquestaci√≥n de contenedores
‚îÇ
‚îú‚îÄ‚îÄ src/                     # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ master_server.py     # Servidor Master (Sistema A)
‚îÇ   ‚îú‚îÄ‚îÄ collector_server.py  # Servidor Collector (Sistema B)
‚îÇ   ‚îú‚îÄ‚îÄ genome_worker.py     # Celery Worker (CPU-bound)
‚îÇ   ‚îú‚îÄ‚îÄ monitor_agent.py     # Agente Monitor (IPC + M√©tricas)
‚îÇ   ‚îú‚îÄ‚îÄ submit_job.py        # Cliente CLI para enviar trabajos
‚îÇ   ‚îú‚îÄ‚îÄ query_job.py         # Cliente CLI para consultar estado
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.py       # L√≥gica de divisi√≥n en chunks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ protocol.py      # Definici√≥n de mensajes JSON
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py        # Configuraci√≥n de logging
‚îÇ   ‚îÇ

‚îÇ
‚îú‚îÄ‚îÄ tests/                   # Tests unitarios e integraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ test_master.py
‚îÇ   ‚îú‚îÄ‚îÄ test_collector.py
‚îÇ   ‚îú‚îÄ‚îÄ test_worker.py
‚îÇ   ‚îú‚îÄ‚îÄ test_agent.py
‚îÇ   ‚îî‚îÄ‚îÄ test_integration.py
‚îÇ
‚îú‚îÄ‚îÄ data/                    # Datos de ejemplo
‚îÇ   ‚îú‚îÄ‚îÄ genome_sample.txt    # Genoma de ejemplo (200MB)
‚îÇ   ‚îî‚îÄ‚îÄ patterns.txt         # Patrones de b√∫squeda
‚îÇ
‚îú‚îÄ‚îÄ logs/                    # Directorio de logs (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ master.log
‚îÇ   ‚îú‚îÄ‚îÄ collector.log
‚îÇ   ‚îú‚îÄ‚îÄ workers.log
‚îÇ   ‚îî‚îÄ‚îÄ alerts.log
‚îÇ
‚îî‚îÄ‚îÄ docs/                    # Documentaci√≥n adicional
    ‚îú‚îÄ‚îÄ diagrams/
    ‚îÇ   ‚îú‚îÄ‚îÄ architecture.png
    ‚îÇ   ‚îî‚îÄ‚îÄ sequence.png
    ‚îú‚îÄ‚îÄ performance.md       # An√°lisis de performance
    ‚îî‚îÄ‚îÄ troubleshooting.md   # Gu√≠a de resoluci√≥n de problemas
```

---

## 9. Casos de Uso y Escenarios

### 10.1 Caso de Uso 1: An√°lisis Gen√≥mico Simple

**Objetivo:** Buscar patr√≥n `AGGTCCAT` en genoma de 200MB.

**Pasos:**
```bash
# 1. Levantar sistema
docker-compose up -d

# 2. Esperar a que todos los servicios est√©n listos (~10 segundos)
docker-compose ps

# 3. Enviar trabajo
python src/submit_job.py \
  --server localhost \
  --port 5000 \
  --file data/genome_sample.txt \
  --pattern "AGGTCCAT"

# Output:
# Job submitted successfully!
# Job ID: 550e8400-e29b-41d4-a716-446655440000
# Total chunks: 4096
# Estimated time: ~120 seconds

# 4. Consultar progreso
python src/query_job.py \
  --server localhost \
  --port 5000 \
  --job-id 550e8400-e29b-41d4-a716-446655440000

# Output:
# Job Status: PROCESSING
# Progress: 2048/4096 chunks (50.00%)
# Matches found so far: 87
# Active workers: 3

# 5. Esperar a que termine
# (repetir comando anterior hasta ver Status: COMPLETED)

# 6. Ver resultado final
python src/query_job.py \
  --server localhost \
  --port 5000 \
  --job-id 550e8400-e29b-41d4-a716-446655440000 \
  --show-results

# Output:
# Job Status: COMPLETED
# Total matches: 142
# Processing time: 118.5 seconds
# Average time per chunk: 0.029 seconds
# Workers used: 3
```

### 10.2 Caso de Uso 2: Simulaci√≥n de Fallo de Worker

**Objetivo:** Demostrar que el sistema detecta y reporta cuando un worker cae.

**Pasos:**
```bash
# 1. Sistema funcionando normally
docker-compose up -d

# 2. Enviar trabajo grande
python src/submit_job.py --file data/genome_sample.txt --pattern "AGGTCCAT"

# 3. Mientras procesa, matar un worker
docker kill genome-worker2

# 4. Observar logs del Collector
docker-compose logs -f collector

# Output esperado:
# [2024-10-30 12:34:56] WARNING: Worker worker2 no reporta hace 15 segundos
# [2024-10-30 12:35:11] CRITICAL: Worker worker2 Status=DEAD
# [2024-10-30 12:35:12] INFO: Alerta enviada al Master
# [2024-10-30 12:35:12] INFO: Tareas de worker2 re-encoladas

# 5. Observar logs del Master
docker-compose logs -f master

# Output esperado:
# [2024-10-30 12:35:12] WARNING: Recibida notificaci√≥n de worker_down: worker2
# [2024-10-30 12:35:13] INFO: Encontradas 47 tareas pendientes de worker2
# [2024-10-30 12:35:14] INFO: Re-encolando tareas...
# [2024-10-30 12:35:15] INFO: 47 tareas re-encoladas exitosamente

# 6. Verificar que el trabajo se completa con los workers restantes
python src/query_job.py --job-id <uuid>
# Status: COMPLETED (con solo 2 workers activos)
```

### 10.3 Caso de Uso 3: Monitoreo de M√©tricas en Tiempo Real

**Objetivo:** Ver m√©tricas de los workers mientras procesan.

**Pasos:**
```bash
# 1. Conectarse a Redis para ver m√©tricas
docker exec -it genome-redis redis-cli

# 2. Ver estado de todos los workers
127.0.0.1:6379> KEYS worker:*:metrics
1) "worker:worker1:metrics"
2) "worker:worker2:metrics"
3) "worker:worker3:metrics"

# 3. Ver m√©tricas de un worker espec√≠fico
127.0.0.1:6379> GET worker:worker1:metrics
"{\"worker_id\": \"worker1\", \"cpu_percent\": 95.2, \"memory_percent\": 42.1, \"status\": \"ALIVE\"}"

# 4. Ver alertas generadas
127.0.0.1:6379> LRANGE alerts:history 0 -1

# 5. Ver estado de un job
127.0.0.1:6379> GET job:550e8400-e29b-41d4-a716-446655440000:status
"processing"

127.0.0.1:6379> LLEN job:550e8400-e29b-41d4-a716-446655440000:tasks
(integer) 4096

127.0.0.1:4096> LLEN job:550e8400-e29b-41d4-a716-446655440000:results
(integer) 2048
```

---

## 10. Consideraciones de Implementaci√≥n

### 11.1 Gesti√≥n de Chunks con Overlap

**Problema:** Si un patr√≥n de ADN cae justo en la frontera entre dos chunks, puede no detectarse.

**Soluci√≥n:** Agregar overlap entre chunks.

**Post-procesamiento:** Al agregar resultados, eliminar duplicados en zonas de overlap usando las posiciones absolutas.

### 11.2 Manejo de Conexiones Persistentes vs Ef√≠meras

**Decisi√≥n de Dise√±o:**

- **Cliente ‚Üí Master:** Conexi√≥n ef√≠mera (1 request = 1 conexi√≥n)
- **Agente ‚Üí Collector:** Conexi√≥n ef√≠mera pero frecuente (cada 10 segundos)
- **Collector ‚Üí Master:** Conexi√≥n ef√≠mera solo cuando hay alerta
- **Worker ‚Üî Agente (IPC):** Socket persistente (el Agente escucha, Worker conecta cuando necesita)

**Justificaci√≥n:** Para I/O-bound con asyncio, conexiones ef√≠meras son suficientes y m√°s simples. No necesitamos WebSockets ni conexiones persistentes para este proyecto.

### 11.3 Formato de Archivo Genoma

**Formato esperado:** Texto plano ASCII con caracteres `A`, `C`, `G`, `T` (nucle√≥tidos).

```
# Ejemplo: data/genome_sample.txt
AGGTCCATAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTACGATCGATCGATCG
TAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC
AGGTCCATAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTACGATCGATCGATCG
...
```

**Generaci√≥n del archivo de 200MB:**
```bash
# Script para generar genoma sint√©tico
python scripts/generate_genome.py --size 200 --output data/genome_sample.txt
```

### 11.4 Algoritmo de B√∫squeda de Patrones

**Opciones:**

1. **Regex simple:** `re.finditer(pattern, text)` - O(n*m) peor caso, pero suficiente para el proyecto
2. **KMP:** O(n+m) - M√°s eficiente, pero m√°s complejo
3. **Boyer-Moore:** O(n/m) caso promedio - El m√°s r√°pido para patrones largos

**Decisi√≥n:** Usar **regex** por simplicidad. Si se necesita optimizar, implementar KMP.

### 11.5 Estrategia de Re-encolado

**Situaci√≥n:** Worker cae con tareas asignadas pero no completadas.

**Estrategia (Aprovechando features de Celery):**

Celery proporciona mecanismos autom√°ticos de re-encolado cuando se configura correctamente:

1. **`ack_late=True`**: Worker confirma tarea DESPU√âS de completarla (no antes)
2. **`reject_on_worker_lost=True`**: Si worker cae, Celery autom√°ticamente re-encola la tarea
3. El Master solo necesita:
   - Recibir notificaci√≥n del Collector cuando un worker cae
   - Loggear el evento para auditor√≠a
   - Opcionalmente, marcar en Redis: `worker:{id}:status = DEAD`

**Implementaci√≥n:**

**En el Master:**

**Nota para implementaci√≥n:** El sistema de re-encolado autom√°tico de Celery es robusto y bien probado. Para este proyecto acad√©mico, aprovechar estas features es m√°s profesional que implementar l√≥gica custom que puede tener bugs.

### 11.6 Logging Estructurado

**Est√°ndar:** Todos los logs en formato JSON para f√°cil parsing.

**Uso:**

### 11.7 Seguridad y Validaci√≥n

**Validaciones Necesarias:**

1. **Tama√±o de archivo:** Rechazar archivos > 500MB
2. **Patr√≥n:** Solo caracteres A, C, G, T
3. **Rate limiting:** Max 10 jobs por cliente por minuto
4. **Sanitizaci√≥n:** Validar todos los inputs JSON

---

## 11. M√©tricas de Performance Esperadas

### 13.1 Benchmarks Objetivo

Con la configuraci√≥n de 3 workers (4 cores cada uno):

| M√©trica | Valor Esperado |
|------------|---------------|
| **Tiempo de procesamiento (200MB)** | 90-150 segundos |
| **Throughput** | ~1.5-2 MB/s |
| **Chunks por segundo** | ~30-40 |
| **Latencia de heartbeat (IPC)** | < 10ms |
| **Latencia de reporte de m√©tricas** | < 100ms |
| **Tiempo de detecci√≥n de fallo** | 15-20 segundos |
| **Overhead de monitoring** | < 5% CPU por agente |

### 13.2 Factores que Afectan Performance

1. **Tama√±o de chunk:** 50KB es √≥ptimo (m√°s peque√±o ‚Üí overhead, m√°s grande ‚Üí desbalance)
2. **Overlap:** 100 bytes es suficiente (patrones de ADN t√≠picos < 50 bases)
3. **Concurrencia de Celery:** 4 procesos por worker es √≥ptimo para CPUs de 4 cores
4. **Algoritmo de b√∫squeda:** Regex es O(n*m), KMP ser√≠a O(n+m) pero m√°s complejo

### 13.3 C√≥mo Medir Performance

---

## 12. Troubleshooting

### 15.1 Problemas Comunes

**Problema:** Workers no pueden conectarse a Redis
```
Error: ConnectionError: Error 111 connecting to redis:6379
```
**Soluci√≥n:**
```bash
# Verificar que Redis est√° corriendo
docker-compose ps redis

# Verificar red de Docker
docker network inspect genome-network

# Restart de Redis
docker-compose restart redis
```

---

**Problema:** IPC socket no existe
```
Error: [Errno 2] No such file or directory: '/tmp/worker_1.sock'
```
**Soluci√≥n:**
```bash
# Verificar que el Agente est√° corriendo antes que el Worker
# En docker-compose.yml, el Agente debe iniciar primero:
command: >
  sh -c "
    python src/monitor_agent.py ... & 
    sleep 2  # Esperar a que Agente cree el socket
    celery -A src.genome_worker worker ...
  "
```

---

**Problema:** Celery no encuentra las tareas
```
Error: Received unregistered task of type 'tasks.find_pattern'
```
**Soluci√≥n:**

---

**Problema:** Master no puede dividir archivo grande
```
Error: MemoryError: Unable to allocate array
```
**Soluci√≥n:**

---

**Problema:** Collector recibe m√©tricas duplicadas
```
Warning: Received metrics from worker_1 twice in 1 second
```
**Soluci√≥n:**

### 15.2 Logs de Debugging

**Habilitar logs detallados:**
```bash
# En docker-compose.yml, agregar:
environment:
  - LOG_LEVEL=DEBUG
  - CELERY_LOG_LEVEL=DEBUG
```

**Ver logs estructurados:**
```bash
# Filtrar logs JSON por nivel
docker-compose logs master | jq 'select(.level=="ERROR")'

# Ver solo mensajes
docker-compose logs collector | jq '.message'

# Contar alertas por worker
docker-compose logs collector | jq 'select(.message | contains("ALERT"))' | jq -r '.worker_id' | sort | uniq -c
```

---

## 13. Contacto y Contribuciones

**Autor:** [Tu Nombre]  
**Materia:** Computaci√≥n II - [Universidad]  
**Fecha:** Octubre 2025

**Para consultas sobre el proyecto:**
- Email: [tu-email]
- GitHub: [tu-repo]

---

## Ap√©ndices

### Ap√©ndice A: Comandos √ötiles de Docker

```bash
# Ver uso de recursos de contenedores
docker stats

# Ejecutar comando en contenedor
docker exec -it genome-master bash

# Ver variables de entorno
docker exec genome-worker1 env

# Inspeccionar red
docker network inspect genome-network

# Ver vol√∫menes
docker volume ls
docker volume inspect genome_redis-data

# Limpiar todo (cuidado!)
docker-compose down -v
docker system prune -a
```

### Ap√©ndice B: Comandos √ötiles de Redis

```bash
# Conectarse a Redis
docker exec -it genome-redis redis-cli

# Ver todas las keys
KEYS *

# Ver tama√±o de una key
MEMORY USAGE job:uuid:tasks

# Ver info de Redis
INFO

# Ver estad√≠sticas de Celery
LLEN celery  # Tareas pendientes
KEYS celery-task-meta-*

# Limpiar todo (cuidado!)
FLUSHALL
```

### Ap√©ndice C: Comandos √ötiles de Celery

```bash
# Ver workers activos
celery -A src.genome_worker inspect active

# Ver estad√≠sticas
celery -A src.genome_worker inspect stats

# Ver tareas registradas
celery -A src.genome_worker inspect registered

# Purgar todas las tareas
celery -A src.genome_worker purge

# Ver workers conectados
celery -A src.genome_worker inspect ping
```

### Ap√©ndice D: Glosario de T√©rminos

- **Chunk:** Fragmento del archivo genoma (t√≠picamente 50KB)
- **Overlap:** Bytes duplicados entre chunks consecutivos
- **Heartbeat:** Se√±al peri√≥dica que indica que un proceso est√° vivo
- **IPC:** Inter-Process Communication (comunicaci√≥n entre procesos)
- **Unix Domain Socket:** Socket para IPC en la misma m√°quina
- **Celery:** Framework de colas de tareas distribuidas
- **Broker:** Servidor que maneja la cola de mensajes (Redis)
- **Worker:** Proceso que consume y ejecuta tareas
- **Master:** Servidor coordinador del grid de c√≥mputo
- **Collector:** Servidor que recolecta m√©tricas de monitoreo
- **Agente:** Proceso que monitorea un worker local
```