# Proyecto Final: üß¨ Analisis de Genoma Humano üß¨

**Estado:**  Documento Definitivo de Arquitectura v2.0

Este repositorio contiene el proyecto final para la materia Computaci√≥n II. Es un sistema distribuido desarrollado en Python que implementa dos subsistemas paralelos e interconectados:

1. **Sistema A - Grid de C√≥mputo:** Procesa tareas CPU-bound (an√°lisis gen√≥mico) de forma paralela y distribuida.
2. **Sistema B - Sistema de Monitoreo:** Vigila la salud de los nodos del grid en tiempo real.

---

## üìñ √çndice

* [1. Visi√≥n General del Proyecto](#1-visi√≥n-general-del-proyecto)
* [2. Arquitectura del Sistema](#2-arquitectura-del-sistema)
* [3. Componentes Detallados](#3-componentes-detallados)
* [4. Protocolos de Comunicaci√≥n](#4-protocolos-de-comunicaci√≥n)
* [5. Cumplimiento de Requisitos](#5-cumplimiento-de-requisitos)
* [6. Core del Stack Tecnol√≥gico](#6-core-del-stack-tecnol√≥gico)
* [7. Despliegue con Docker üê≥](#7-despliegue-con-docker-)
* [8. Consideraciones de Implementaci√≥n](#8-consideraciones-de-implementaci√≥n)
* [9. Scripts de Utilidad](#9-scripts-de-utilidad)
* [10. Contacto y Contribuciones](#10-contacto-y-contribuciones)

## Glosario de T√©rminos ü§ì

- **Chunk:** Fragmento del archivo genoma (t√≠picamente 1MB)
- **Overlap:** Bytes duplicados entre chunks consecutivos
- **Heartbeat:** Se√±al peri√≥dica que indica que un proceso est√° vivo
- **IPC:** Inter-Process Communication (comunicaci√≥n entre procesos)
- **Unix Domain Socket:** Socket para IPC en la misma m√°quina
- **Celery:** Framework de colas de tareas distribuidas
- **Broker:** Servidor que maneja la cola de mensajes (Redis)
- **Worker:** Proceso que consume y ejecuta tareas
- **Master:** Servidor coordinador del grid de c√≥mputo
- **Collector:** Servidor que recolecta m√©tricas de monitoreo
- **Agente:** Proceso que monitorea un worker local

---

## 1. Visi√≥n General del Proyecto

### 1.1 El Concepto: "El Director y El Vigilante"

El proyecto consiste en dos sistemas distribuidos que operan simult√°neamente:

**üéº El Director (Sistema A - Grid de C√≥mputo)**
- **Rol:** Coordinar la ejecuci√≥n paralela de tareas computacionales pesadas
- **Responsabilidad:** Garantizar que el *trabajo* se complete correctamente
- **Ejemplo:** Analizar Genoma Humano, buscando patrones espec√≠ficos

**üëÅÔ∏è El Vigilante (Sistema B - Monitoreo)**
- **Rol:** Supervisar la salud e infraestructura de los nodos de c√≥mputo
- **Responsabilidad:** Garantizar que los *workers* est√©n operativos y saludables
- **Ejemplo:** Detectar cuando un worker cae, se cuelga o consume recursos anormales

### 1.2 Caso de Uso Principal: An√°lisis Gen√≥mico

**Problema:** Buscar todas las ocurrencias de un patr√≥n de ADN (ej: `AGGTCCAT`) en un archivo de secuencia gen√≥mica de 200MB.

**Soluci√≥n Distribuida:**
1. Dividir el archivo en ~200 chunks de ~1MB cada uno
2. Distribuir los chunks a m√∫ltiples workers (5 por defecto) para procesamiento paralelo
3. Mientras procesan, monitorear su estado de salud en tiempo real
4. Agregar los resultados parciales en un resultado final
5. Persistir el resultado y estad√≠sticas en Redis

**Por qu√© este caso de uso:**
- Es CPU-bound (justifica paralelizaci√≥n)
- Es divisible (justifica grid computing)
- Exige recursos (demuestra necesidad de monitoreo)

---
## 2. Arquitectura del Sistema

### 2.1 Diagramas de Arquitectura

Para una comprensi√≥n visual de la arquitectura, consulte los siguientes diagramas:

*   **Arquitectura General:** [Diagrama del Sistema Distribuido](docs/diagramas/diagrama-logico.png)
*   **Arquitectura de Contenedores (Docker):** [Diagrama de Contenedores](#71-arquitectura-de-contenedores)

### 2.2 Topolog√≠a de Red

**Conexiones TCP (Sockets):**
1. `Cliente CLI` ‚Üî `Master` (puerto 5000)
2. `Agentes` ‚Üí `Collector` (puerto 6000)
3. `Collector` ‚Üí `Master` (puerto 5000)

**Conexiones IPC (Unix Domain Sockets):**
- `Worker` ‚Üî `Agente` (mismo contenedor/host)

**Redis:**
- `Master`, `Workers` ‚Üí Redis (puerto 6379)

### 2.3 Flujo de la Informaci√≥n

**Comunicaci√≥n Local (IPC - dentro del mismo contenedor):**
- Worker ‚Üî Agente: Unix Domain Socket para heartbeats constantes (no solo durante ejecucion)
- Procesos Celery internos: `multiprocessing.Queue` y `Lock` (solo dentro de cada Worker)

**Comunicaci√≥n Distribuida (TCP - entre contenedores):**
- Cliente ‚Üí Master: Sockets TCP (env√≠o de trabajos)
- Agente ‚Üí Collector: Sockets TCP (reporte de m√©tricas)
- Collector ‚Üí Master: Sockets TCP (notificaciones de alertas)

**Comunicaci√≥n v√≠a Redis:**
- Master ‚Üí Workers: Celery Queue. DB I (distribuci√≥n de chunks)
- Workers ‚Üí Redis: Almacenamiento de resultados parciales. DB II

```
[Cliente] ---(TCP)---> [Master] ---(Redis)---> [Workers]
                          ‚ñ≤                        ‚îÇ
                          ‚îÇ                        ‚îÇ (IPC local)
                          ‚îÇ                        ‚ñº
                          ‚îÇ                   [Agentes]
                          ‚îÇ                        ‚îÇ
                          ‚îÇ                        ‚îÇ (TCP)
                          ‚îÇ                        ‚ñº
                          ‚îî‚îÄ‚îÄ(TCP alerta)‚îÄ‚îÄ‚îÄ‚îÄ [Collector]
```

---

## 3. Componentes Detallados

### 3.1 Cliente CLI (`submit_job.py`)

**Prop√≥sito:** Interfaz de l√≠nea de comandos para enviar trabajos al grid.

**Tecnolog√≠as:**
- `argparse` para parseo de argumentos
- `socket` para conexi√≥n TCP al Master
- `json` para serializaci√≥n de mensajes

**Argumentos:**
```bash
python submit_job.py \
  --server localhost \
  --port 5000 \
  --file genome.txt \
  --pattern "AGGTCCAT" \
```

**Protocolo de Comunicaci√≥n:**
```json
// REQUEST (Cliente -> Master)
{
  "type": "submit_job",
  "job_id": "uuid-generado-por-cliente",
  "filename": "genome.txt",
  "pattern": "AGGTCCAT",
  "chunk_size": 1048576,
  "file_size": 209715200,
  "file_data_b64": "base64_encoded_data..."  // Archivo codificado en base64
}

// RESPONSE (Master -> Cliente)
{
  "status": "accepted",
  "job_id": "uuid-generado-por-cliente",
  "total_chunks": 200,
  "estimated_time": 120  // segundos
}
```

**Funcionalidades:**
- Validar que el archivo existe
- Calcular tama√±o y estimar chunks
- Enviar archivo al Master
- Recibir confirmaci√≥n con `job_id`

---

### 3.2 Cliente CLI (`query_job.py`)

**Prop√≥sito:** Interfaz de l√≠nea de comandos para consultar el estado de un trabajo en el grid.

**Tecnolog√≠as:**
- `argparse` para parseo de argumentos
- `socket` para conexi√≥n TCP al Master
- `json` para serializaci√≥n de mensajes

**Argumentos:**
```bash
python src/query_job.py \
  --server localhost \
  --port 5000 \
  --job-id "uuid-del-trabajo" \
  --show-results \
  --output-html "reporte.html"
```

**Protocolo de Comunicaci√≥n:**
El request y response para la consulta de estado ya est√°n definidos en la secci√≥n `4.2 Mensajes Cliente ‚Üî Master`.

**Funcionalidades:**
- Consultar el estado de un trabajo mediante su `job_id`.
- Mostrar el progreso del trabajo en la consola.
- Opcionalmente, mostrar los resultados finales si el trabajo ha sido completado.
- Opcionalmente, generar un reporte HTML con el estado detallado del trabajo.

---

### 3.3 Servidor Master (`master_server.py`)

**Prop√≥sito:** Orquestador central del sistema de c√≥mputo.

**Tecnolog√≠as:**
- `asyncio` para servidor as√≠ncrono
- `sockets` (via `asyncio.start_server`) para conexiones TCP
- `celery` para encolar tareas en Redis (DB I)
- `redis` (librer√≠a de gestion con python) para persistencia de datos

**Responsabilidades:**

1.  **Recepci√≥n de Trabajos:**
    *   Escucha en el puerto 5000.
    *   Acepta m√∫ltiples clientes concurrentemente.
    *   Valida el formato JSON del trabajo.

2.  **Creaci√≥n y Encolado del Trabajo:**
    *   Crea un hash en Redis para almacenar toda la informaci√≥n y el estado del trabajo (`job:{job_id}`).
    *   Divide el archivo en bloques de **1MB** (configurable en `settings.py`).
    *   Agrega un solapamiento (overlap) entre chunks para no perder patrones en las fronteras.
    *   Por cada chunk, encola una tarea en Celery: `tasks.find_pattern.delay(...)`.

3.  **Consulta de Estado:**
    *   Responde a las peticiones del cliente (`query_job.py`) sobre el estado de un trabajo, leyendo la informaci√≥n directamente del hash de Redis.

4.  **Comunicaci√≥n con Collector:**
    *   Recibe notificaciones de workers ca√≠dos y las registra. El re-encolado de tareas es gestionado autom√°ticamente por Celery.

**Estructura del C√≥digo:**

**Persistencia en Redis:**
El Master crea un **√∫nico hash por trabajo** que centraliza toda la informaci√≥n. Los workers actualizan este hash de forma at√≥mica.

```
# Hash principal del trabajo
job:{job_id} (HASH)
  - "status": "processing" | "completed" | "failed"
  - "total_chunks": 200,
  - "processed_chunks": 0 (incrementado por los workers)
  - "matches_found": 0 (incrementado por los workers)
  - "filename": "genome.txt"
  - ... (otros metadatos)

# Lista de resultados detallados (actualmente no utilizada por el cliente)
job:{job_id}:results (LIST) -> [json_result_chunk_1, json_result_chunk_2, ...]
```

---

### 3.4 Celery Worker (`genome_worker.py`)

**Prop√≥sito:** Procesar chunks de datos (CPU-bound).

**Tecnolog√≠as:**
- `celery` como framework de workers
- `re` expresion regular para b√∫squeda de patrones (funcion: `re.finditer`)
- `redis` para actualizaci√≥n de estado

**Responsabilidades:**
1.  **Procesamiento de Chunks:**
    *   Recibe un chunk de datos y un patr√≥n a buscar.
    *   Encuentra todas las ocurrencias del patr√≥n en el chunk.
2.  **Actualizaci√≥n At√≥mica en Redis:**
    *   Usa `HINCRBY` para incrementar at√≥micamente los contadores `processed_chunks` y `matches_found` en el hash `job:{job_id}`.
    *   Esto asegura que el progreso se actualiza de forma concurrente y sin generar bloqueos.
3.  **Comunicaci√≥n con Agente:**
    *   Inicia un hilo en segundo plano para enviar un `heartbeat` al Agente Monitor local cada 5 segundos v√≠a Unix Socket.


**Configuraci√≥n:**
- Concurrencia: 5 procesos por worker 
- Prefetch: 2 (no acaparar tareas)
- Ack_late: True (reencolar si worker cae antes de completar tarea)

**Comando de inicio:**
```bash
celery -A genome_worker worker \
  --loglevel=info \
  --concurrency=4 \
  --hostname=worker1@%h
```

---

### 3.5 Agente Monitor (`monitor_agent.py`)

**Prop√≥sito:** Monitorear la salud del worker en su misma m√°quina/contenedor.

**Tecnolog√≠as:**
- `psutil` para m√©tricas del sistema
- `socket` (*Unix* Domain Socket) para IPC con Worker
- `socket` (*TCP*) para reportar al Collector
- `asyncio` para loop principal

**Responsabilidades:**

1. **Recolecci√≥n de M√©tricas:**
   - CPU usage (`cpu_percent`)
   - Memory usage (`memory_percent`)

2. **Comunicaci√≥n IPC con Worker:**
   - Crear Unix Domain Socket: `/tmp/worker_{id}.sock`
   - Escuchar heartbeats del worker cada 5 segundos
   - Si no recibe heartbeat en 15 segundos ‚Üí reportar como **DEAD** al Worker

3. **Reporte al Collector:**
   - Cada 10 segundos enviar m√©tricas
   - Si detecta estado DEAD, enviar *alerta inmediata*

**Nota importante:** El Worker de Celery con `--concurrency=4` lanza 4 procesos hijos. Cada uno puede enviar heartbeats al mismo Unix socket. El Agente, usando `asyncio.start_unix_server()`, acepta m√∫ltiples conexiones concurrentes sin problema.

---

### 3.6 Servidor Collector (`collector_server.py`)

**Prop√≥sito:** Centralizar monitoreo de todos los workers y generar alertas.

**Tecnolog√≠as:**
- `asyncio` para servidor as√≠ncrono
- `sockets` *TCP* para recibir m√©tricas de Agentes y enviar alertas al Master

**Responsabilidades:**

1.  **Recepci√≥n de M√©tricas:**
    *   Escucha en el puerto 6000 y acepta conexiones de m√∫ltiples agentes.
    *   Mantiene un registro en memoria del √∫ltimo `heartbeat` recibido de cada worker.

2.  **Detecci√≥n de Anomal√≠as:**
    *   Si un agente reporta `status = "DEAD"`, se considera una alerta cr√≠tica.
    *   Inicia un monitor en segundo plano que comprueba cada 15 segundos si alg√∫n worker ha dejado de enviar m√©tricas (timeout de 30 segundos).

3.  **Generaci√≥n de Alertas:**
    *   Ante una anomal√≠a cr√≠tica, registra el evento en los logs.
    *   Notifica directamente al Master Server v√≠a TCP para que registre el incidente.

---

## 4. Protocolos de Comunicaci√≥n

### 4.1 Formato de Mensajes JSON

Todos los mensajes entre componentes usan **JSON** con **protocolos** definidos en `protocol.py`.
Estos protocolos siguen la siguiente estructura base:

```json
{
  "type": "message_type",
  "timestamp": 1234567890.123,
  "data": { ... }
}
```

### 4.2 Mensajes Cliente ‚Üî Master

**Submit Job:**
```json
{
  "type": "submit_job",
  "job_id": "uuid-generado-por-cliente",
  "filename": "genome.txt",
  "pattern": "AGGTCCAT",
  "chunk_size": 51200,
  "file_size": 209715200,
  "file_data_b64": "..."
}
```

**Job Status Query:**
```json
{
  "type": "query_status",
  "job_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

**Response:**
```json
{
  "status": "processing",
  "job_id": "uuid-generado-por-cliente",
  "progress": {
    "total_chunks": 200,
    "processed_chunks": 2048,
    "percentage": 50.0
  },
  "partial_results": {
    "matches_found": 42
  }
}
```

### 4.3 Mensajes Agente ‚Üí Collector

**Metrics Report:**
```json
{
  "type": "metrics",
  "data": {
    "worker_id": "worker_1",
    "timestamp": 1234567890.123,
    "status": "ALIVE",
    "cpu_percent": 87.5,
    "memory_mb": 1024.5,
    "memory_percent": 45.2,
    "tasks_processed": 128
  }
}
```

### 4.4 Mensajes Collector ‚Üí Master

**Worker Down Notification:**
```json
{
  "type": "worker_down",
  "worker_id": "worker_3",
  "timestamp": 1234567890.123,
  "last_task_id": "abc123-task-id"
}
```

### 4.5 IPC Worker ‚Üî Agente (Unix Socket)

**Heartbeat:**
```json
{
  "type": "heartbeat",
  "timestamp": 1234567890.123,
  "tasks_completed": 10
}
```

---

## 5. Cumplimiento de Requisitos

### 5.1 Requisitos Obligatorios de la C√°tedra

| Requisito | Implementaci√≥n | Justificaci√≥n |
|-----------|----------------|---------------|
| **Sockets con m√∫ltiples clientes** | `asyncio.start_server()` en Master y Collector | Master maneja N clientes CLI concurrentemente. Collector maneja N agentes. No se usa framework web, sino sockets directos. |
| **Asincronismo I/O** | `asyncio` con `async/await` | Cr√≠tico para que Master y Collector manejen cientos de conexiones sin bloquearse. Permite I/O concurrente eficiente. |
| **Cola de tareas distribuidas** | `Celery + Redis` | (1) Workers procesan chunks CPU-bound. (2) Collector encola alertas I/O-bound. Demuestra versatilidad del patr√≥n. |
| **Mecanismos IPC** | Unix Domain Sockets | Worker y Agente (procesos distintos, mismo host) se comunican v√≠a socket Unix para heartbeats. Es IPC puro, no red. |
| **Parseo de argumentos CLI** | `argparse` | Cliente, Master, Collector, Worker, Agente: todos usan `argparse` para configuraci√≥n (--port, --host, --worker-id, etc.). |
| **Soporte Dual-Stack (IPv4/IPv6)** | `socket.getaddrinfo` | Los clientes y servidores utilizan `getaddrinfo` para resolver un hostname a una lista de posibles direcciones (IPv4 e IPv6) e intentan conectarse a la primera que funcione, garantizando compatibilidad en distintas configuraciones de red. |

### 5.2 Requisitos Adicionales Implementados

- **Docker/Docker Compose:** Despliegue completo y orquestado de todo el sistema, garantizando un entorno de ejecuci√≥n consistente y facilitando la escalabilidad.
- **Persistencia con Redis:** Uso de Redis no solo como broker de Celery, sino como una base de datos clave-valor para almacenar el estado, progreso y resultados de los trabajos.
- **Logging Estructurado (JSON):** Todos los componentes generan logs en formato JSON, lo que facilita el an√°lisis, la b√∫squeda y la integraci√≥n con sistemas de monitoreo centralizado.
- **Manejo Robusto de Errores:** Implementaci√≥n de bloques `try-except` en todas las operaciones de red e IPC para evitar ca√≠das inesperadas y registrar informaci√≥n √∫til para el debugging.
- **M√©tricas del Sistema con `psutil`:** Recolecci√≥n de m√©tricas de uso de CPU y memoria en tiempo real para el monitoreo de la salud de los nodos de c√≥mputo.
- **Protocolo JSON Validado:** Todos los mensajes intercambiados entre componentes siguen un protocolo estricto definido y validado con `protocol.py`, asegurando la integridad de la comunicaci√≥n.
- **Tolerancia a Fallos y Alta Disponibilidad:** El sistema es resiliente a la ca√≠da de workers. Gracias a la configuraci√≥n de Celery (`ack_late=True`), las tareas no confirmadas se re-encolan autom√°ticamente para ser procesadas por otros workers disponibles.
- **Escalabilidad Horizontal:** La arquitectura permite escalar f√°cilmente el poder de c√≥mputo a√±adiendo m√°s instancias de workers
- **Actualizaciones de Estado At√≥micas:** Los workers actualizan el progreso de los trabajos en Redis usando operaciones at√≥micas (`HINCRBY`), lo que previene condiciones de carrera y garantiza la consistencia de los datos sin necesidad de bloqueos.

---

## 6. Core del Stack Tecnol√≥gico

| Componente | Versi√≥n | Uso |
|------------|---------|-----|
| **Python** | 3.11+ | Lenguaje principal |
| **asyncio** | stdlib | Servidores as√≠ncronos (Master, Collector) |
| **sockets** | stdlib | Comunicaci√≥n de red de bajo nivel |
| **Celery** | 5.3+ | Cola de tareas distribuidas |
| **Redis** | 7.0+ | (1) Broker Celery, (2) Persistencia, (3) Estado |
| **multiprocessing** | stdlib | IPC con Unix Domain Sockets |
| **argparse** | stdlib | CLI parsing |
| **psutil** | 5.9+ | M√©tricas del sistema |
| **Docker** | 24+ | Contenedorizaci√≥n |

Mas info en `requirements.txt`

---

## 7. Despliegue con Docker üê≥

El sistema est√° completamente dockerizado para garantizar la portabilidad y facilitar el despliegue. Se utiliza Docker Compose para orquestar los diferentes servicios (Master, Collector, Workers, Redis).

Para una explicaci√≥n detallada de las decisiones de dise√±o, la configuraci√≥n de la red, los vol√∫menes y los comandos espec√≠ficos de cada servicio, consulte el documento [**Explicaci√≥n de Docker en el Proyecto (`docs/DOCKER_EXPLAIN.md`)**](docs/DOCKER_EXPLAIN.md).

### 7.1 Arquitectura de Contenedores

El siguiente diagrama ilustra c√≥mo los diferentes servicios interact√∫an como contenedores dentro de la red de Docker:

![Diagrama de Contenedores Docker](docs/diagramas/diagrama%20contenedores.png)

### 7.2 Comandos de Despliegue

```bash
# Build de todas las im√°genes
docker-compose build

# Levantar todo el sistema
docker-compose up -d

# Ver logs en tiempo real
docker-compose logs -f

# Ver logs de un componente espec√≠fico
docker-compose logs -f master
docker-compose logs -f worker1

# Escalar workers (agregar m√°s)
docker-compose up -d --scale worker=5

# Detener todo
docker-compose down

# Detener y limpiar vol√∫menes
docker-compose down -v
```

---

## 8. Consideraciones de Implementaci√≥n

### 8.1 Gesti√≥n de Chunks con Overlap

**Problema:** Si un patr√≥n de ADN cae justo en la frontera entre dos chunks, puede no detectarse.

**Soluci√≥n:** Agregar overlap entre chunks.

**Post-procesamiento:** Al agregar resultados, eliminar duplicados en zonas de overlap usando las posiciones absolutas.

### 8.2 Manejo de Conexiones Persistentes vs Ef√≠meras

**Decisi√≥n de Dise√±o:**

- **Cliente ‚Üí Master:** Conexi√≥n ef√≠mera (1 request = 1 conexi√≥n)
- **Agente ‚Üí Collector:** Conexi√≥n ef√≠mera pero frecuente (cada 10 segundos)
- **Collector ‚Üí Master:** Conexi√≥n ef√≠mera solo cuando hay alerta
- **Worker ‚Üî Agente (IPC):** Socket persistente (el Agente escucha, Worker conecta cuando necesita)

**Justificaci√≥n:** Para I/O-bound con asyncio, conexiones ef√≠meras son suficientes y m√°s simples. No necesitamos WebSockets ni conexiones persistentes para este proyecto.

### 8.3 Formato de Archivo Genoma

**Formato esperado:** Texto plano ASCII con caracteres `A`, `C`, `G`, `T` (nucle√≥tidos).

```
# Ejemplo: data/genome_sample.txt
AGGTCCATAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTACGATCGATCGATCG
TAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC
AGGTCCATAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTACGATCGATCGATCG
...
```

**Generaci√≥n del archivo de 200MB:**
```bash
# Script para generar genoma sint√©tico
python scripts/generate_genome.py --size 200 --output data/genome_sample.txt
```

### 8.4 Algoritmo de B√∫squeda de Patrones

**Opciones:**

1. **Regex simple:** `re.finditer(pattern, text)` - O(n*m) peor caso, pero suficiente para el proyecto
2. **KMP:** O(n+m) - M√°s eficiente, pero m√°s complejo
3. **Boyer-Moore:** O(n/m) caso promedio - El m√°s r√°pido para patrones largos

**Decisi√≥n:** Usar **regex** por simplicidad. Si se necesita optimizar, implementar KMP.

### 8.5 Estrategia de Re-encolado

**Situaci√≥n:** Worker cae con tareas asignadas pero no completadas.

**Estrategia (Aprovechando features de Celery):**

Celery proporciona mecanismos autom√°ticos de re-encolado cuando se configura correctamente:

1. **`ack_late=True`**: Worker confirma tarea DESPU√âS de completarla (no antes)
2. **`reject_on_worker_lost=True`**: Si worker cae, Celery autom√°ticamente re-encola la tarea
3. El Master solo necesita:
   - Recibir notificaci√≥n del Collector cuando un worker cae
   - Loggear el evento para auditor√≠a
   - Opcionalmente, marcar en Redis: `worker:{id}:status = DEAD`

### 8.6 Logging Estructurado

**Est√°ndar:** Todos los logs en formato JSON para f√°cil parsing e interpretacion.

### 8.7 Seguridad y Validaci√≥n

**Validaciones Necesarias:**

1. **Tama√±o de archivo:** Rechazar archivos > 500MB
2. **Patr√≥n:** Solo caracteres A, C, G, T
3. **Rate limiting:** Max 10 jobs por cliente por minuto
4. **Sanitizaci√≥n:** Validar todos los inputs JSON

### 8.8 Troubleshooting y Debugging
Pueden habilitarse logs mas detallados para realizar un debugging

**Habilitar logs detallados:**
```bash
# En docker-compose.yml, agregar:
environment:
  - LOG_LEVEL=DEBUG
  - CELERY_LOG_LEVEL=DEBUG
```

---

## 9. Scripts de Utilidad

Para facilitar el desarrollo, las pruebas y el despliegue, el proyecto incluye varios scripts de utilidad en el directorio `scripts/`.

### 9.1 Script de Ejecuci√≥n de Prueba (`scripts/ejecucionCLI.sh`)

Este script automatiza un flujo de trabajo completo para probar la funcionalidad principal del sistema:
1.  Env√≠a un nuevo trabajo de an√°lisis gen√≥mico usando `submit_job.py`.
2.  Captura el `job_id` devuelto por el servidor.
3.  Consulta el estado de ese trabajo en un bucle usando `query_job.py` hasta que se completa.
4.  Una vez completado, genera un reporte final en formato HTML.

**Uso:**
```bash
./scripts/ejecucionCLI.sh
```

### 9.2 Script de Limpieza de Docker (`scripts/restart_docker_clean.sh`)

Este es un script de utilidad para desarrolladores que **limpia completamente el entorno de Docker**. Es √∫til cuando se necesita un reinicio total del sistema para evitar problemas de cach√©.

**¬°Advertencia!** Este script es destructivo. Eliminar√°:
- Todos los contenedores (en ejecuci√≥n y detenidos).
- Todas las im√°genes de Docker.
- Todos los vol√∫menes no utilizados (incluyendo el de Redis si no est√° en uso).
- Todas las redes personalizadas.

Despu√©s de la limpieza, reconstruye y levanta el sistema con `docker compose up -d --build`.

**Uso:**
```bash
./scripts/restart_docker_clean.sh
```

### 9.3 Script de Instalaci√≥n de Dependencias (`scripts/install_requirements.sh`)

Este script facilita la configuraci√≥n de un entorno de desarrollo local **sin usar Docker**.
1.  Crea un entorno virtual de Python en `venv/`.
2.  Activa el entorno virtual.
3.  Instala todas las dependencias listadas en `requirements.txt`.

**Uso:**
```bash
./scripts/install_requirements.sh
```

---

## 10. Contacto y Contribuciones

**Autor:** Martin Salvatore
**Materia:** Computaci√≥n II - Universidad de Mendoza  
**Fecha:** Noviembre 2025

**Para consultas sobre el proyecto:**
- Email: martingsalvatore@gmail.com